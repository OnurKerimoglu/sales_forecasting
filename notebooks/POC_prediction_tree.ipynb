{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Pipeline: Tree-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import parquet\n",
    "\n",
    "from tree_predictor import TreePredictor\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.. Done.\n",
      "Fixing data schemas.. Done.\n",
      "Cleaning training data\n",
      "Checking for negative values in ['price', 'amount'].. Count of rows marked as invalid: 6469\n",
      "Checking for outliers in ['price', 'amount'].. Count of rows marked as invalid: 40138\n",
      "Count of cleaned rows: 46248\n",
      "Cleaning validation data\n",
      "Checking for negative values in ['price', 'amount'].. Count of rows marked as invalid: 1507\n",
      "Count of cleaned rows: 1507\n",
      "Prepared daily raw data.\n"
     ]
    }
   ],
   "source": [
    "config = Utils.read_config_for_env(config_path='../config/config.yml')\n",
    "treepredictor = TreePredictor(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare monthly training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: when ready, move this to tree_predictor\n",
    "def data_prep_pipeline(\n",
    "        df_daily,\n",
    "        splitname,\n",
    "        refresh):\n",
    "    \n",
    "    # get base monthly data\n",
    "    df_base = treepredictor.data.get_monthly_data(df_daily, splitname, refresh)\n",
    "\n",
    "    # get monthly data with lag and ma features\n",
    "    df_ts= treepredictor.data.get_ts_data(df_base, splitname, refresh, treepredictor.num_lag_mon)\n",
    "    \n",
    "    return df_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/Onur/opt/MLrepos/shop_sales_prediction/data/train_base.parquet\n",
      "Loading /Users/Onur/opt/MLrepos/shop_sales_prediction/data/train_ts.parquet\n"
     ]
    }
   ],
   "source": [
    "columns_needed = ['monthly_period', 'shop_id', 'item_id', 'item_category_id', 'amount', 'price']\n",
    "df_daily_train = treepredictor.df_daily_train[columns_needed].copy()\n",
    "# df_daily_train.info()\n",
    "df_train = data_prep_pipeline(\n",
    "    df_daily_train,\n",
    "    'train',\n",
    "    refresh=False)\n",
    "\n",
    "# create X and y\n",
    "y_train = df_train['amount_item']\n",
    "df_train.drop(columns=['price', 'amount_item', 'amount_cat'], axis=1, inplace=True)\n",
    "X_train = df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/Onur/opt/MLrepos/shop_sales_prediction/data/val_base.parquet\n",
      "Loading /Users/Onur/opt/MLrepos/shop_sales_prediction/data/val_ts.parquet\n"
     ]
    }
   ],
   "source": [
    "df_daily_val = treepredictor.df_daily_val[columns_needed].copy()\n",
    "# df_daily_train.info()\n",
    "df_val = data_prep_pipeline(\n",
    "    df_daily_val,\n",
    "    'val',\n",
    "    refresh=False)\n",
    "\n",
    "# create X and y\n",
    "y_val = df_val['amount_item']\n",
    "df_val.drop(columns=['price', 'amount_item', 'amount_cat'], axis=1, inplace=True)\n",
    "X_val= df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# from lightgbm import LGBMRegressor \n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error as mse \n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LightGBM dataset for training with features X_train and labels Y_train \n",
    "train_data = lgb.Dataset(X_train, label=y_train) \n",
    "# Create a LightGBM dataset for testing with features X_val and labels Y_val, \n",
    "# and specify the reference dataset as train_data for consistent evaluation \n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data) \n",
    "# Define a dictionary of parameters for configuring the LightGBM regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.466542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2610\n",
      "[LightGBM] [Info] Number of data points in the train set: 31924800, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 0.078381\n",
      "[1]\tvalid_0's rmse: 1.65917\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's rmse: 1.64354\n",
      "[3]\tvalid_0's rmse: 1.62982\n",
      "[4]\tvalid_0's rmse: 1.61804\n",
      "[5]\tvalid_0's rmse: 1.60776\n",
      "[6]\tvalid_0's rmse: 1.59815\n",
      "[7]\tvalid_0's rmse: 1.59186\n",
      "[8]\tvalid_0's rmse: 1.58492\n",
      "[9]\tvalid_0's rmse: 1.57901\n",
      "[10]\tvalid_0's rmse: 1.57547\n",
      "[11]\tvalid_0's rmse: 1.57279\n",
      "[12]\tvalid_0's rmse: 1.56934\n",
      "[13]\tvalid_0's rmse: 1.56753\n",
      "[14]\tvalid_0's rmse: 1.5655\n",
      "[15]\tvalid_0's rmse: 1.56399\n",
      "[16]\tvalid_0's rmse: 1.56182\n",
      "[17]\tvalid_0's rmse: 1.55994\n",
      "[18]\tvalid_0's rmse: 1.55767\n",
      "[19]\tvalid_0's rmse: 1.55633\n",
      "[20]\tvalid_0's rmse: 1.55509\n",
      "[21]\tvalid_0's rmse: 1.55393\n",
      "[22]\tvalid_0's rmse: 1.55317\n",
      "[23]\tvalid_0's rmse: 1.5525\n",
      "[24]\tvalid_0's rmse: 1.55177\n",
      "[25]\tvalid_0's rmse: 1.55112\n",
      "[26]\tvalid_0's rmse: 1.5505\n",
      "[27]\tvalid_0's rmse: 1.54985\n",
      "[28]\tvalid_0's rmse: 1.54912\n",
      "[29]\tvalid_0's rmse: 1.54857\n",
      "[30]\tvalid_0's rmse: 1.54804\n",
      "[31]\tvalid_0's rmse: 1.54783\n",
      "[32]\tvalid_0's rmse: 1.54713\n",
      "[33]\tvalid_0's rmse: 1.5468\n",
      "[34]\tvalid_0's rmse: 1.5461\n",
      "[35]\tvalid_0's rmse: 1.54626\n",
      "[36]\tvalid_0's rmse: 1.54552\n",
      "[37]\tvalid_0's rmse: 1.546\n",
      "[38]\tvalid_0's rmse: 1.54554\n",
      "[39]\tvalid_0's rmse: 1.54543\n",
      "[40]\tvalid_0's rmse: 1.54477\n",
      "[41]\tvalid_0's rmse: 1.54447\n",
      "[42]\tvalid_0's rmse: 1.5449\n",
      "[43]\tvalid_0's rmse: 1.54477\n",
      "[44]\tvalid_0's rmse: 1.54472\n",
      "[45]\tvalid_0's rmse: 1.54443\n",
      "[46]\tvalid_0's rmse: 1.54447\n",
      "[47]\tvalid_0's rmse: 1.54446\n",
      "[48]\tvalid_0's rmse: 1.54443\n",
      "[49]\tvalid_0's rmse: 1.54409\n",
      "[50]\tvalid_0's rmse: 1.54423\n",
      "[51]\tvalid_0's rmse: 1.54455\n",
      "[52]\tvalid_0's rmse: 1.54434\n",
      "[53]\tvalid_0's rmse: 1.54396\n",
      "[54]\tvalid_0's rmse: 1.54388\n",
      "[55]\tvalid_0's rmse: 1.54369\n",
      "[56]\tvalid_0's rmse: 1.54367\n",
      "[57]\tvalid_0's rmse: 1.54386\n",
      "[58]\tvalid_0's rmse: 1.54383\n",
      "[59]\tvalid_0's rmse: 1.54386\n",
      "[60]\tvalid_0's rmse: 1.54376\n",
      "[61]\tvalid_0's rmse: 1.54361\n",
      "[62]\tvalid_0's rmse: 1.54362\n",
      "[63]\tvalid_0's rmse: 1.54367\n",
      "[64]\tvalid_0's rmse: 1.54367\n",
      "[65]\tvalid_0's rmse: 1.54361\n",
      "[66]\tvalid_0's rmse: 1.54376\n",
      "[67]\tvalid_0's rmse: 1.54383\n",
      "[68]\tvalid_0's rmse: 1.54379\n",
      "[69]\tvalid_0's rmse: 1.54379\n",
      "[70]\tvalid_0's rmse: 1.54377\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid_0's rmse: 1.54361\n"
     ]
    }
   ],
   "source": [
    "params = { \n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 30,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "}\n",
    "callback_early_stopping = lgb.early_stopping(5)\n",
    "num_round = 100\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_round,\n",
    "    valid_sets=[val_data],\n",
    "    callbacks=[callback_early_stopping, lgb.log_evaluation()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE:  0.547277644868107\n",
      "Validation RMSE:  1.54360680064235\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the training and validation data. \n",
    "pred_train = model.predict(X_train)\n",
    "pred_val = model.predict(X_val)\n",
    "\n",
    "# Calculate and print the Root Mean Squared Error (RMSE) for training and validation predictions. \n",
    "print(\"Training RMSE: \", np.sqrt(mse(y_train, pred_train)))\n",
    "print(\"Validation RMSE: \", np.sqrt(mse(y_val, pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.28733265883206"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
